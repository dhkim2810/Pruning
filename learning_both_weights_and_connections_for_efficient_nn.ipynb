{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os, sys\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(message)s\", datefmt=\"%m-%d %H:%M\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((2,2))\n",
    "\n",
    "    def forward(self, x, mode=None):\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def converged(old, new):\n",
    "    converge = True\n",
    "    for old_score, new_score in zip(old, new):\n",
    "        converge = converge and abs(old_score - new_score) < 0.001\n",
    "    return converge"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Data Loader\n",
    "def get_data():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(), normalize])\n",
    "    valid_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                    datasets.CIFAR10(root=\"./data\",train=True, transform=train_transform,download=True),\n",
    "                    batch_size=256, num_workers=4, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "                    datasets.CIFAR10(root=\"./data\",train=False, transform=train_transform,download=True),\n",
    "                    batch_size=256, num_workers=4, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Utilities\n",
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, *meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "    \n",
    "    def print(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "    \n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "def adjust_learning_rate(optimizer, lr, verbose=False):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    if verbose:\n",
    "        print(optimizer.param_groups)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', dir=None, is_best=False):\n",
    "    if dir is not None and not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    filename = filename if dir is None else os.path.join(dir, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = 'model_best.pth.tar'\n",
    "        if dir is not None:\n",
    "            bestname = os.path.join(dir, bestname)\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar', dir=None):\n",
    "    assert dir is None or os.path.exists(dir)\n",
    "    if dir:\n",
    "        filename = os.path.join(dir, filename)\n",
    "    return torch.load(filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Train function\n",
    "def train(train_loader, **kwargs):\n",
    "    epoch = kwargs.get('epoch')\n",
    "    model = kwargs.get('model')\n",
    "    criterion = kwargs.get('criterion')\n",
    "    optimizer = kwargs.get('optimizer')\n",
    "    cuda = kwargs.get('cuda')\n",
    "\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(len(train_loader), batch_time, data_time, losses, top1, top5, prefix=\"Epoch:[{}]\".format(epoch))\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if cuda:\n",
    "            input = Variable(input).cuda()\n",
    "            target = Variable(target).cuda()\n",
    "        else:\n",
    "            input = Variable(input)\n",
    "            target = Variable(target)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1,5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(acc1[0], input.size(0))\n",
    "        top5.update(acc5[0], input.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "\n",
    "        if i % 100==0:\n",
    "            progress.print(i)\n",
    "    logging.info('====> Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, top5.avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Validation function\n",
    "def validate(val_loader, model, criterion, cuda=False):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(len(val_loader), batch_time, losses, top1, top5, prefix='Test: ')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            end = time.time()\n",
    "            if cuda:\n",
    "                input = Variable(input).cuda()\n",
    "                target = Variable(target).cuda()\n",
    "            else:\n",
    "                input = Variable(input)\n",
    "                target = Variable(target)\n",
    "\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1,5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(acc1[0], input.size(0))\n",
    "            top5.update(acc5[0], input.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                progress.print(i)\n",
    "        logging.info('====> Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg, top5.avg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training Configuration\n",
    "cuda = True\n",
    "model = AlexNet()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=5e-3, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader, test_loader = get_data()\n",
    "\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phase 1. Learn important connections"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top1 = 0\n",
    "top5 = 0\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch : {}, lr : {}\".format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    print('===> [ Training ]')\n",
    "    acc1_train, acc5_train = train(train_loader,\n",
    "                            epoch=epoch, model=model,\n",
    "                            criterion=criterion, optimizer=optimizer, cuda=cuda)\n",
    "\n",
    "    print('===> [ Validation ]')\n",
    "    acc1_valid, acc5_valid = validate(test_loader, model, criterion, cuda)\n",
    "\n",
    "    chk = {\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'epoch' : epoch,\n",
    "        'optimizer' : optimizer.state_dict()\n",
    "    }\n",
    "    save_checkpoint(chk, dir='checkpoint', is_best=(top5 < acc5_valid))\n",
    "\n",
    "    top1 = max(acc1_valid, top1)\n",
    "    top5 = max(acc5_valid, top5)\n",
    "\n",
    "    scheduler.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phase 2~3. Prune and Retraining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Compressor(object):\n",
    "    def __init__(self, model, cuda=False):\n",
    "        self.model = model\n",
    "        self.num_layers = 0\n",
    "        self.num_dropout_layers = 0\n",
    "        self.dropout_rates = {}\n",
    "\n",
    "        self.count_layers()\n",
    "\n",
    "        self.weight_masks = [None for _ in range(self.num_layers)]\n",
    "        self.bias_masks = [None for _ in range(self.num_layers)]\n",
    "\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def count_layers(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                self.num_layers += 1\n",
    "            elif isinstance(m, nn.Dropout):\n",
    "                self.dropout_rates[self.num_dropout_layers] = m.p\n",
    "                self.num_dropout_layers += 1\n",
    "\n",
    "    def prune(self):\n",
    "        '''\n",
    "        :return: percentage pruned in the network\n",
    "        '''\n",
    "        index = 0\n",
    "        dropout_index = 0\n",
    "\n",
    "        num_pruned, num_weights = 0, 0\n",
    "\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                num = torch.numel(m.weight.data)\n",
    "\n",
    "                if type(m) == nn.Conv2d:\n",
    "                    if index == 0:\n",
    "                        alpha = 0.015\n",
    "                    else:\n",
    "                        alpha = 0.2\n",
    "                else:\n",
    "                    if index == self.num_layers - 1:\n",
    "                        alpha = 0.25\n",
    "                    else:\n",
    "                        alpha = 1\n",
    "\n",
    "                # use a byteTensor to represent the mask and convert it to a floatTensor for multiplication\n",
    "                weight_mask = torch.ge(m.weight.data.abs(), alpha * m.weight.data.std()).type('torch.FloatTensor')\n",
    "                if self.cuda:\n",
    "                    weight_mask = weight_mask.cuda()\n",
    "                self.weight_masks[index] = weight_mask\n",
    "\n",
    "                bias_mask = torch.ones(m.bias.data.size())\n",
    "                if self.cuda:\n",
    "                    bias_mask = bias_mask.cuda()\n",
    "\n",
    "                # for all kernels in the conv2d layer, if any kernel is all 0, set the bias to 0\n",
    "                # in the case of linear layers, we search instead for zero rows\n",
    "                for i in range(bias_mask.size(0)):\n",
    "                    if len(torch.nonzero(weight_mask[i]).size()) == 0:\n",
    "                        bias_mask[i] = 0\n",
    "                self.bias_masks[index] = bias_mask\n",
    "\n",
    "                index += 1\n",
    "\n",
    "                layer_pruned = num - torch.nonzero(weight_mask).size(0)\n",
    "                logging.info('number pruned in weight of layer %d: %.3f %%' % (index, 100 * (layer_pruned / num)))\n",
    "                bias_num = torch.numel(bias_mask)\n",
    "                bias_pruned = bias_num - torch.nonzero(bias_mask).size(0)\n",
    "                logging.info('number pruned in bias of layer %d: %.3f %%' % (index, 100 * (bias_pruned / bias_num)))\n",
    "\n",
    "                num_pruned += layer_pruned\n",
    "                num_weights += num\n",
    "\n",
    "                m.weight.data *= weight_mask\n",
    "                m.bias.data *= bias_mask\n",
    "\n",
    "            elif isinstance(m, nn.Dropout):\n",
    "                # update the dropout rate\n",
    "                mask = self.weight_masks[index - 1]\n",
    "                m.p = self.dropout_rates[dropout_index] * math.sqrt(torch.nonzero(mask).size(0) \\\n",
    "                                             / torch.numel(mask))\n",
    "                dropout_index += 1\n",
    "                logging.info(\"new Dropout rate: %4e\" % m.p)\n",
    "\n",
    "        # print(self.weight_masks)\n",
    "        return num_pruned / num_weights\n",
    "\n",
    "\n",
    "    def set_grad(self):\n",
    "        # print(self.weight_masks)\n",
    "        index = 0\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "                m.weight.grad.data *= self.weight_masks[index]\n",
    "                m.bias.grad.data *= self.bias_masks[index]\n",
    "                index += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compressor = Compressor(model, cuda=cuda)\n",
    "lr = 1e-4 # 1/10 of training\n",
    "adjust_learning_rate(optimizer, lr, verbose=False)\n",
    "\n",
    "model.train()\n",
    "pruned_percentage = 0.0\n",
    "prune_score = [AverageMeter(f'Acc@{k}', ':6.2f') for k in (1,5)]\n",
    "validation_score = [0.0 for _ in (1,5)]\n",
    "\n",
    "# Iterative Pruning\n",
    "epoch = 1\n",
    "max_epoch = 5\n",
    "while True:\n",
    "    if epoch == 1:\n",
    "        new_pruned_percentage = compressor.prune()\n",
    "        logging.info('Pruned %.3f %%' % (100 * new_pruned_percentage))\n",
    "        acc = validate(test_loader, model, criterion, cuda=cuda)\n",
    "        if new_pruned_percentage - pruned_percentage <= 0.001 and converged(validation_score, acc):\n",
    "            break\n",
    "        pruned_percentage = new_pruned_percentage\n",
    "        validation_score = acc\n",
    "    for e in range(epoch, max_epoch+1):\n",
    "        acc1_train, acc5_train = train(train_loader,\n",
    "                            epoch=e, model=model,\n",
    "                            criterion=criterion, optimizer=optimizer, cuda=cuda)\n",
    "    epoch = 1\n",
    "final_top1, final_top5 = validate(test_loader, model, criterion, cuda=cuda)\n",
    "logging.info(f\"Iterative Pruning Final Result : Top1 %.3f / Top5 %.3f\" % final_top1, final_top5)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('3.8.8': pyenv)"
  },
  "interpreter": {
   "hash": "25951d8d7320e86f23b9e984592385cc4e6384d81423a052a7f5d0effe8b0e17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}